from server.llm_clients.groq_client import get_llm_client
from server.prompt_templates.templates import direct_answer_prompt
from server.schemas.llm_schemas import DirectLLMResponse


def generate_direct_answer_from_llm(query: str) -> str:
    """
    Generate a direct answer to a user query using an LLM.

    This function initializes an LLM client with a specified model,
    sets up a structured output schema for the response,
    constructs a prompt chain, and invokes the chain with the user's question.
    The function expects the LLM to return a JSON object matching the DirectLLMResponse schema.

    Args:
        query (str): The user's question to be answered by the LLM.

    Returns:
        str: The answer generated by the LLM.
    """
    # Initialize the LLM client with the desired model
    client = get_llm_client(model_name="llama-3.3-70b-versatile")

    # Configure the LLM to expect a structured output matching DirectLLMResponse
    llm = client.with_structured_output(DirectLLMResponse, method="json_mode")

    # Create the prompt chain by combining the direct answer prompt and the LLM
    chain = direct_answer_prompt | llm

    # Invoke the chain with the user's question and get the structured response
    response = chain.invoke({"question": query})

    # Return the answer field from the structured response
    return response.answer
